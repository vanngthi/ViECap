{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/van-n/.envs/conda-envs/viecap/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/DATA/van-n/.envs/conda-envs/viecap/lib/python3.11/importlib/__init__.py:126: UserWarning: A NumPy version >=1.25.2 and <2.6.0 is required for this version of SciPy (detected version 1.24.1)\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n",
      "Generating train split: 13481 examples [00:00, 29354.25 examples/s]\n",
      "Generating validation split: 4620 examples [00:00, 30156.49 examples/s]\n",
      "Generating test split: 1155 examples [00:00, 29055.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dset = load_dataset(\"SEACrowd/uit_viic\", \"uit_viic_seacrowd_imtext\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'image_paths', 'texts', 'metadata'],\n",
      "        num_rows: 13481\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'image_paths', 'texts', 'metadata'],\n",
      "        num_rows: 4620\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'image_paths', 'texts', 'metadata'],\n",
      "        num_rows: 1155\n",
      "    })\n",
      "})\n",
      "{'id': '4990', 'image_paths': ['http://images.cocodataset.org/train2017/000000157656.jpg', 'http://farm3.staticflickr.com/2566/3796096212_bd02c4c56c_z.jpg'], 'texts': 'Người đàn ông đang đánh tennis ngoài sân.', 'metadata': {'context': '', 'labels': [0]}}\n"
     ]
    }
   ],
   "source": [
    "print(dset)\n",
    "print(dset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "This tutorial demonstrates how to prepare the data in advance. If you would like to utilize our trained model, you can simply download the annotations and checkpoints from [Release](https://github.com/FeiElysia/ViECap/releases/tag/checkpoints) and skip the data preparation section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the code with your custom dataset, you need to preprocess your dataset initially. This preprocessing will generate a list containing captions. For instance, considering the Flickr30k dataset, the training dataset is structured as a dictionary, where the image name serves as the key and the corresponding captions are the values. You should transform the data format into a list: [caption 1, caption 2, ..., caption n]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../annotations/flickr30k/train_captions.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m             captions.append(caption)\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m captions\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m captions = \u001b[43mload_flickr30k_captions\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../annotations/flickr30k/train_captions.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe type of the processed caption is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(captions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, the total training samples are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(captions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     23\u001b[39m captions[:\u001b[32m10\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mload_flickr30k_captions\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_flickr30k_captions\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[38;5;28mstr\u001b[39m]:   \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[32m      6\u001b[39m         annotations = json.load(infile) \u001b[38;5;66;03m# dictionary -> {image_path: List[caption1, caption2, ...]}\u001b[39;00m\n\u001b[32m      7\u001b[39m     punctuations = [\u001b[33m'\u001b[39m\u001b[33m!\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m#\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m$\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m&\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m(\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m+\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m<\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m>\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m@\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m[\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m]\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m^\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m|\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m}\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m~\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DATA/van-n/.envs/conda-envs/viecap/lib/python3.11/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../annotations/flickr30k/train_captions.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "def load_flickr30k_captions(path: str) -> List[str]:   \n",
    "    with open(path, 'r') as infile:\n",
    "        annotations = json.load(infile) # dictionary -> {image_path: List[caption1, caption2, ...]}\n",
    "    punctuations = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', ' ', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "    captions = []\n",
    "    for image_path in annotations:                  \n",
    "        temp_captions = annotations[image_path]\n",
    "        for caption in temp_captions:\n",
    "            caption = caption.strip()\n",
    "            if caption.isupper():\n",
    "                caption = caption.lower()\n",
    "            caption = caption[0].upper() + caption[1:]\n",
    "            if caption[-1] not in punctuations:\n",
    "                caption += '.'\n",
    "            captions.append(caption)\n",
    "    return captions\n",
    "\n",
    "captions = load_flickr30k_captions('../annotations/flickr30k/train_captions.json')\n",
    "print(f'The type of the processed caption is: {type(captions)}, the total training samples are: {len(captions)}')\n",
    "captions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining the correct format for the training dataset, you can proceed to execute the following function from ```entities_extraction.py``` to pre-extract entities for each caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def entities_extraction(captions: List[str], path: str) -> None:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        captions: [caption 1, caption 2, ..., caption n]\n",
    "        path: the output path of training data with the format of List[List[List, str]] i.e., [[[entity1, entity2,...], caption], ...]  \n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_captions = []\n",
    "    for caption in captions:\n",
    "        detected_entities = []\n",
    "        pos_tags = nltk.pos_tag(nltk.word_tokenize(caption))\n",
    "        for entities_with_pos in pos_tags:\n",
    "            if entities_with_pos[1] == 'NN' or entities_with_pos[1] == 'NNS':\n",
    "                entity = lemmatizer.lemmatize(entities_with_pos[0].lower().strip())\n",
    "                detected_entities.append(entity)\n",
    "        detected_entities = list(set(detected_entities))\n",
    "        new_captions.append([detected_entities, caption])\n",
    "    \n",
    "    with open(path, 'wb') as outfile:\n",
    "        pickle.dump(new_captions, outfile)\n",
    "\n",
    "captions = captions[:20] # take the first 20 captions as an example\n",
    "outpath = './training_set_with_entities.pickle'\n",
    "entities_extraction(captions, outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can check the generated file, which contains entities for each training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of the processed file is: <class 'list'>, the total training samples are: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['guy', 'look', 'yard', 'hand', 'hair'],\n",
       "  'Two young guys with shaggy hair look at their hands while hanging out in the yard.'],\n",
       " [['bush', 'male'], 'Two young, White males are outside near many bushes.'],\n",
       " [['shirt', 'yard', 'men'], 'Two men in green shirts are standing in a yard.'],\n",
       " [['man', 'shirt', 'garden'], 'A man in a blue shirt standing in a garden.'],\n",
       " [['friend', 'time'], 'Two friends enjoy time spent together.'],\n",
       " [['system', 'pulley', 'hat', 'men'],\n",
       "  'Several men in hard hats are operating a giant pulley system.'],\n",
       " [['piece', 'worker', 'equipment'],\n",
       "  'Workers look down from up above on a piece of equipment.'],\n",
       " [['machine', 'hat', 'men'],\n",
       "  'Two men working on a machine wearing hard hats.'],\n",
       " [['structure', 'top', 'men'], 'Four men on top of a tall structure.'],\n",
       " [['rig', 'men'], 'Three men on a large rig.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(outpath, 'rb') as infile:\n",
    "    captions_with_entities = pickle.load(infile)\n",
    "print(f'The type of the processed file is: {type(captions_with_entities)}, the total training samples are: {len(captions_with_entities)}')\n",
    "captions_with_entities[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the training process, run the following function from ```texts_features_extraction.py``` to pre-extract text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "\n",
    "@torch.no_grad()\n",
    "def texts_features_extraction(device: str, clip_type: str, inpath: str, outpath: str):\n",
    "\n",
    "    device = device\n",
    "    encoder, _ = clip.load(clip_type, device)\n",
    "\n",
    "    with open(inpath, 'rb') as infile:\n",
    "        captions_with_entities = pickle.load(infile) # [[[entity1, entity2, ...], caption], ...]\n",
    "\n",
    "    for idx in range(len(captions_with_entities)):\n",
    "        caption = captions_with_entities[idx][1]\n",
    "        tokens = clip.tokenize(caption, truncate = True).to(device)\n",
    "        embeddings = encoder.encode_text(tokens).squeeze(dim = 0).to('cpu')\n",
    "        captions_with_entities[idx].append(embeddings)\n",
    "    \n",
    "    with open(outpath, 'wb') as outfile:\n",
    "        pickle.dump(captions_with_entities, outfile)\n",
    "    \n",
    "    return captions_with_entities\n",
    "\n",
    "device = 'cuda:0'\n",
    "clip_type = 'ViT-B/32'\n",
    "inpath = './training_set_with_entities.pickle'\n",
    "outpath = './training_set_texts_features_ViT-B32.pickle'\n",
    "text_features = texts_features_extraction(device, clip_type, inpath, outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the extracted text features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of the extracted text features are: <class 'list'>, the total training samples are: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['guy', 'look', 'yard', 'hand', 'hair'],\n",
       " 'Two young guys with shaggy hair look at their hands while hanging out in the yard.',\n",
       " tensor([ 1.7798e-01,  1.9336e-01, -3.8989e-01, -3.6011e-01,  1.0211e-01,\n",
       "          1.4661e-01,  4.4312e-01, -2.3450e-01, -1.2268e-01, -4.8364e-01,\n",
       "          1.5894e-01,  8.6548e-02, -3.8623e-01, -3.8989e-01, -3.1055e-01,\n",
       "          2.3108e-01, -3.9337e-02, -2.2290e-01,  2.0496e-01,  2.6147e-01,\n",
       "          3.0444e-01, -6.5063e-02,  1.4771e-01, -5.9424e-01,  2.3877e-01,\n",
       "         -1.6724e-01,  6.1890e-02,  4.0088e-01,  1.4014e-01, -1.8225e-01,\n",
       "          9.3323e-02, -2.4365e-01,  2.4414e-01, -4.4586e-02, -6.9775e-01,\n",
       "          1.3428e-01, -2.5195e-01, -6.6162e-02,  2.7466e-02,  6.5430e-02,\n",
       "         -2.9395e-01,  9.3933e-02,  2.2168e-01, -8.6670e-03,  2.4390e-01,\n",
       "          2.0483e-01,  3.3813e-01,  4.4800e-02,  7.6172e-02,  1.5601e-01,\n",
       "          1.2817e-01,  3.6401e-01,  8.7524e-02, -5.1904e-01, -2.6025e-01,\n",
       "          7.3471e-03, -4.1895e-01,  6.6338e-03, -2.9517e-01,  4.7607e-02,\n",
       "          1.5808e-01, -3.2666e-01,  2.4036e-01, -6.6223e-02,  4.4409e-01,\n",
       "         -1.2413e-02,  8.4900e-02,  8.5840e-01, -2.5684e-01, -2.2583e-01,\n",
       "          2.3767e-01,  3.5840e-01, -4.1687e-02,  4.7913e-02,  1.9580e-01,\n",
       "          6.3965e-02, -5.4321e-02,  1.1450e-01,  1.5234e-01, -6.1084e-01,\n",
       "          1.4906e-03,  3.0615e-01,  2.2034e-01, -2.2803e-01, -2.1265e-01,\n",
       "         -2.0129e-01, -1.0689e-02,  1.2610e-01, -2.8101e-01,  7.1594e-02,\n",
       "          3.7689e-02, -2.6172e-01, -8.0518e-01,  2.6196e-01,  2.6782e-01,\n",
       "          2.9053e-02, -3.6255e-01,  5.7526e-02, -7.6758e-01,  9.8419e-03,\n",
       "         -1.3574e-01,  3.3496e-01,  1.1548e-01, -2.1130e-01, -1.2976e-01,\n",
       "          3.8013e-01, -3.1860e-01,  1.3342e-01,  1.9690e-01, -1.1475e-01,\n",
       "          1.6956e-01,  1.7822e-01, -3.5913e-01,  2.9175e-01, -1.3208e-01,\n",
       "          1.7603e-01, -4.2603e-02, -9.5215e-02, -2.3547e-01, -3.8257e-01,\n",
       "          2.0459e-01, -2.1570e-01, -7.9773e-02,  1.6028e-01,  2.6343e-01,\n",
       "          3.1812e-01,  3.5864e-01,  4.3042e-01, -2.7393e-01, -2.0676e-02,\n",
       "          2.1570e-01,  1.8921e-01, -2.1924e-01,  3.0664e+00, -1.8079e-01,\n",
       "          3.5620e-01, -2.0294e-02, -4.3304e-02, -1.0388e-01, -1.8408e-01,\n",
       "         -2.1509e-01,  3.2080e-01, -1.1761e-01,  1.1334e-01, -2.5439e-01,\n",
       "         -1.1298e-01,  1.3538e-01,  5.4993e-02, -3.8574e-01,  1.4038e-01,\n",
       "          2.6758e-01, -7.4524e-02,  4.7168e-01,  5.7471e-01,  8.0750e-02,\n",
       "         -4.1016e-01,  7.6294e-02,  1.4145e-02, -2.1387e-01, -2.5537e-01,\n",
       "          3.7842e-01,  1.1359e-01, -4.0802e-02, -5.6335e-02, -2.4689e-02,\n",
       "         -8.2275e-02, -5.6396e-01,  2.5742e-02, -3.7445e-02,  6.9763e-02,\n",
       "         -3.0487e-02, -2.8906e-01, -1.9763e-01,  8.4290e-02,  5.1117e-02,\n",
       "          2.5711e-02,  1.4124e-01,  3.2928e-02, -2.8247e-01, -4.6118e-01,\n",
       "         -6.6406e-01,  2.2690e-02, -3.2056e-01,  3.7524e-01, -1.2524e-01,\n",
       "         -1.8848e-01,  1.3745e-01,  2.9678e-02,  1.8501e-04,  5.5957e-01,\n",
       "          4.0527e-01, -2.0630e-02,  3.8818e-01,  8.8562e-02, -3.9062e-01,\n",
       "         -3.6657e-05,  5.4077e-02,  8.5083e-02,  3.2886e-01, -2.3865e-01,\n",
       "         -1.1299e-02, -1.6125e-01, -2.7161e-02,  6.6162e-02,  4.3304e-02,\n",
       "         -3.1543e-01,  2.5171e-01,  1.1884e-01, -2.3303e-01,  1.3062e-01,\n",
       "         -4.7998e-01,  2.2522e-01,  1.5234e-01,  3.7842e-01, -1.5576e-01,\n",
       "         -3.1323e-01,  2.9785e-01, -3.2861e-01,  4.8926e-01, -1.2408e-01,\n",
       "         -4.8975e-01, -2.9956e-01, -5.2719e-03,  1.4038e-01, -4.4482e-01,\n",
       "         -1.0931e-01,  1.1633e-01, -4.7534e-01, -1.0187e-01, -2.8979e-01,\n",
       "         -1.5833e-01,  1.1963e-01,  8.1238e-02, -1.4185e-01,  1.1548e-01,\n",
       "          3.8184e-01, -8.6731e-02, -1.3818e-01, -1.7548e-02, -1.8542e-01,\n",
       "         -6.6223e-03,  2.8174e-01, -1.2585e-01,  2.3962e-01, -6.0107e-01,\n",
       "         -8.1543e-02,  2.1997e-01,  2.6172e-01, -7.5378e-02, -1.3428e-01,\n",
       "          1.5002e-01, -5.2930e-01,  8.8928e-02,  1.6138e-01, -3.4863e-01,\n",
       "         -4.1046e-02, -3.1201e-01,  5.4150e-01,  1.1298e-01,  2.4377e-01,\n",
       "         -1.1688e-01, -2.5269e-02,  1.5271e-01, -3.6597e-01,  6.3538e-02,\n",
       "          2.9083e-02, -2.4963e-02, -7.1716e-02,  4.6191e-01, -1.7395e-02,\n",
       "         -7.3486e-02,  3.8306e-01,  2.4841e-01,  7.5562e-02,  6.3400e-03,\n",
       "         -3.0762e-01, -2.3279e-01,  1.1780e-02,  8.2947e-02, -2.7441e-01,\n",
       "         -2.5635e-01,  1.6113e-01, -3.6108e-01, -4.3628e-01, -4.5746e-02,\n",
       "          5.1709e-01,  8.0261e-02, -8.7708e-02,  2.6807e-01, -1.5955e-01,\n",
       "         -2.1680e-01,  1.2505e-02,  1.6431e-01,  2.1204e-01,  2.0166e-01,\n",
       "          7.2876e-02,  6.1670e-01, -1.6052e-01,  2.4750e-02, -5.6738e-01,\n",
       "         -2.7752e-03, -3.4473e-01,  8.0200e-02,  1.4429e-01, -4.4946e-01,\n",
       "         -3.0396e-01,  1.5479e-01,  3.7988e-01,  1.0565e-01,  4.3726e-01,\n",
       "          2.2253e-01,  1.0944e-01,  3.0586e+00,  1.5161e-01,  1.0229e-01,\n",
       "         -1.3123e-02, -1.1017e-01,  1.8787e-01, -6.6833e-02,  4.6362e-01,\n",
       "         -2.0825e-01,  2.7051e-01, -1.2128e-01, -2.3267e-01, -3.3789e-01,\n",
       "          5.9052e-02,  9.0027e-03, -2.9907e-01,  6.2866e-02, -7.8418e-01,\n",
       "         -1.3611e-01, -1.8213e-01,  1.6187e-01, -1.2988e-01,  1.0785e-01,\n",
       "          6.8909e-02, -5.2002e-01, -1.6382e-01,  1.2842e-01, -7.8186e-02,\n",
       "         -2.1777e-01,  2.2754e-01,  2.9102e-01, -3.0371e-01,  5.5273e-01,\n",
       "         -4.3481e-01,  3.0444e-01, -4.5837e-02,  8.9233e-02, -3.2520e-01,\n",
       "         -2.3645e-01, -9.6313e-02,  2.8046e-02,  1.7053e-01, -2.0251e-01,\n",
       "          5.1367e-01, -2.8149e-01, -2.6245e-01, -4.9512e-01, -2.5879e-01,\n",
       "          1.8054e-01, -3.9893e-01,  5.5176e-02,  2.1777e-01,  3.7292e-02,\n",
       "          2.2009e-01,  2.5488e-01,  5.4199e-01,  8.4961e-02, -6.0181e-02,\n",
       "          1.3855e-01,  4.1992e-02, -2.5928e-01,  7.7820e-02,  3.9478e-01,\n",
       "         -1.4490e-01,  7.3535e-01, -2.0020e-01, -2.3788e-02, -9.1064e-02,\n",
       "          1.3757e-01, -1.0425e-01, -3.1299e-01, -3.1738e-01, -3.8086e-01,\n",
       "          1.2781e-01, -2.9590e-01, -5.3467e-01, -3.1311e-02, -3.0469e-01,\n",
       "         -1.6785e-02, -9.0820e-01,  1.6089e-01,  2.6172e-01,  6.3232e-01,\n",
       "          2.5928e-01,  3.4741e-01,  3.5986e-01,  2.9321e-01,  3.8525e-01,\n",
       "          2.0117e-01, -2.3938e-01, -1.7609e-02,  4.2944e-01, -7.2510e-02,\n",
       "         -5.9326e-01, -3.0249e-01, -5.5115e-02,  2.6733e-01,  4.7339e-01,\n",
       "         -4.0991e-01,  9.9731e-02, -7.3486e-02, -5.1367e-01,  4.3481e-01,\n",
       "         -1.8933e-01,  1.5308e-01, -1.8286e-01,  2.1765e-01, -5.7031e-01,\n",
       "          6.6162e-01,  5.0934e-02,  5.6787e-01, -5.1904e-01,  5.6250e-01,\n",
       "         -5.5127e-01,  1.6251e-02,  3.5425e-01,  3.7939e-01, -8.3008e-02,\n",
       "          3.0444e-01, -1.5540e-01,  1.4145e-02,  3.7537e-02, -1.8176e-01,\n",
       "          2.1912e-01,  2.8906e-01,  1.2067e-01,  1.0876e-01,  3.3960e-01,\n",
       "         -3.5132e-01, -2.8003e-01, -3.3130e-01, -4.6167e-01, -1.9995e-01,\n",
       "         -1.1133e-01, -1.6589e-01,  1.2805e-01, -6.5186e-02, -1.5823e-02,\n",
       "         -3.8818e-01,  2.5830e-01,  8.5510e-02,  2.1973e-01,  1.4050e-01,\n",
       "         -1.7676e-01,  5.0720e-02, -2.4939e-01,  3.9282e-01, -1.2115e-01,\n",
       "          1.4087e-01,  2.8638e-01,  2.8491e-01,  1.9421e-01,  8.6304e-02,\n",
       "         -3.8965e-01,  4.9805e-02,  3.7903e-02, -3.7109e-01, -1.4343e-01,\n",
       "          4.5410e-01, -3.0591e-01, -2.2388e-01, -7.8308e-02,  2.5238e-02,\n",
       "          1.7358e-01, -3.7231e-02,  3.1812e-01,  1.9409e-01,  7.0923e-02,\n",
       "         -1.9922e-01, -3.3154e-01, -5.5084e-02, -2.4207e-01, -1.3525e-01,\n",
       "         -6.2683e-02, -1.7383e-01, -1.4270e-01, -1.6772e-01, -1.0779e-01,\n",
       "          2.3206e-01, -2.2278e-02, -4.3896e-01,  8.8525e-01, -1.8143e-02,\n",
       "          2.7954e-01,  3.6841e-01, -3.0396e-02,  2.2485e-01, -2.4261e-03,\n",
       "          1.1487e-01,  2.0044e-01,  1.1890e-01, -1.6113e-02,  1.7883e-01,\n",
       "         -2.6270e-01, -7.8918e-02,  2.6337e-02,  2.7148e-01,  2.0886e-01,\n",
       "          6.8848e-02,  2.0874e-01], dtype=torch.float16)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'The type of the extracted text features are: {type(captions_with_entities)}, the total training samples are: {len(captions_with_entities)}')\n",
    "text_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this notebook, you can easily make modifications to ```entities_extraction.py```, ```texts_features_extraction.py```, and ```load_annotations.py```, enabling you to train ViECap using your own dataset.\n",
    "\n",
    "To evaluate the trained model on your customized validation set, run the following function from ```images_features_extraction.py``` to pre-extract the image features, ensuring accelerated evaluation. It is advisable to transform your validation set into a dictionary format, where the image name serves as the key and the corresponding caption as the value. Once again, considering the Flickr30k validation set for reference, you can run the following code to check the format of this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of the validation set is: <class 'dict'>, the total validation samples are: 1014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('1018148011.jpg',\n",
       " ['A group of people stand in the back of a truck filled with cotton.',\n",
       "  'Men are standing on and about a truck carrying a white substance.',\n",
       "  'A group of people are standing on a pile of wool in a truck.',\n",
       "  'A group of men are loading cotton onto a truck',\n",
       "  'Workers load sheared wool onto a truck.'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_val_flickr30k = '../annotations/flickr30k/val_captions.json'\n",
    "\n",
    "# format = {image_path: [caption1, caption2, ...]} -> [[image_path, image_features, [caption 1, caption 2, ...]], ...]\n",
    "with open(path_val_flickr30k, 'r') as infile:\n",
    "    val_flickr30k = json.load(infile)\n",
    "\n",
    "print(f'The type of the validation set is: {type(val_flickr30k)}, the total validation samples are: {len(val_flickr30k)}')\n",
    "for key in val_flickr30k:\n",
    "    value = val_flickr30k[key]\n",
    "    break\n",
    "\n",
    "key, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can extract image features using the function from ```images_features_extraction.py``` with datasets in the format aforementioned. If you're using a custom dataset, ensure to adjust the conditional statements in Lines 12-17 of ```images_features_extraction.py```. This will allow you to navigate into the appropriate branch. (i.e., ```if datasets == 'coco' or datasets == 'flickr30k' or datasets == 'your dataset name'``` and ```elif datasets == 'your dataset name': rootpath = 'the path of image source'```)\n",
    "\n",
    "Note that if you choose not to use the provided image features from us, you should download the image source files for the COCO and Flickr30k dataset from their official websites. Afterwards, you should place these files into the 'ViECap/annotations/coco/val2014' directory for COCO images and the 'ViECap/annotations/flickr30k/flickr30k-images' directory for Flickr30k images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def images_features_extraction(datasets, encoder, proprecess, annotations, outpath):\n",
    "    \n",
    "    results = []\n",
    "    if datasets == 'coco' or datasets == 'flickr30k': # coco, flickr30k\n",
    "        # format = {image_path: [caption1, caption2, ...]} -> [[image_path, image_features, [caption1, caption2, ...]], ...]\n",
    "        if datasets == 'coco':\n",
    "            rootpath = '../annotations/coco/val2014/'\n",
    "        elif datasets == 'flickr30k':\n",
    "            rootpath = '../annotations/flickr30k/flickr30k-images/'\n",
    "\n",
    "        flag = 0 # add flag for testing the code\n",
    "        for image_id in annotations:\n",
    "            flag += 1\n",
    "            if flag > 20:\n",
    "                break\n",
    "            caption = annotations[image_id]\n",
    "            image_path = rootpath + image_id\n",
    "            image = proprecess(Image.open(image_path)).unsqueeze(dim = 0).to(device)\n",
    "            image_features = encoder.encode_image(image).squeeze(dim = 0).to('cpu') # clip_hidden_size\n",
    "            results.append([image_id, image_features, caption])\n",
    "\n",
    "    else: # nocaps\n",
    "        # format = [{'split': 'near_domain', 'image_id': '4499.jpg', 'caption': [caption1, caption2, ...]}, ...]\n",
    "        # format = [[image_path, image_split, image_features, [caption1, captions2, ...]], ...]\n",
    "        rootpath = './annotations/nocaps/'\n",
    "        for annotation in annotations:\n",
    "            split = annotation['split']\n",
    "            image_id = annotation['image_id']\n",
    "            caption = annotation['caption']\n",
    "            image_path = rootpath + split + '/' + image_id\n",
    "            image = proprecess(Image.open(image_path)).unsqueeze(dim = 0).to(device)\n",
    "            image_features = encoder.encode_image(image).squeeze(dim = 0).to('cpu') # clip_hidden_size\n",
    "            results.append([image_id, split, image_features, caption])\n",
    "\n",
    "    with open(outpath, 'wb') as outfile:\n",
    "        pickle.dump(results, outfile)\n",
    "\n",
    "encoder, proprecess = clip.load(clip_type, device)\n",
    "outpath = './validation_set_images_features_ViT-B32.pickle'\n",
    "images_features_extraction('flickr30k', encoder, proprecess, val_flickr30k, outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the generated file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of the extracted image features are: <class 'list'>, the total validation samples are: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1018148011.jpg',\n",
       " tensor([-8.8318e-02,  4.5288e-01,  1.1218e-01,  5.0342e-01, -6.2195e-02,\n",
       "          6.6504e-01,  3.8037e-01, -8.9905e-02,  3.9258e-01,  6.2256e-02,\n",
       "         -3.2056e-01, -1.3525e-01,  3.6133e-02, -6.1554e-02,  6.9763e-02,\n",
       "          2.7908e-02, -3.6255e-01, -5.8887e-01, -1.9885e-01,  1.0925e-01,\n",
       "         -8.0957e-01,  1.7700e-01,  8.0109e-03, -5.7666e-01, -3.5376e-01,\n",
       "          4.4580e-01,  9.7900e-02, -6.3965e-01, -4.2542e-02,  3.5303e-01,\n",
       "         -3.3691e-01,  4.5801e-01,  1.0632e-01,  4.4922e-01,  2.7271e-01,\n",
       "          1.6321e-01, -1.4233e-01,  7.3340e-01, -7.0862e-02,  4.5288e-01,\n",
       "          5.7831e-02, -5.8643e-01, -2.0959e-01,  1.1841e-01,  2.6904e-01,\n",
       "          7.7148e-01,  1.0028e-01,  3.1250e-02, -2.8174e-01, -1.5173e-01,\n",
       "          2.6343e-01, -9.6130e-02,  1.3513e-01, -8.7463e-02, -6.1035e-01,\n",
       "          4.5825e-01, -5.6343e-03,  2.2766e-01,  1.9385e-01,  4.0625e-01,\n",
       "         -6.9434e-01,  1.0522e-01,  1.6382e-01,  3.7671e-01, -2.0398e-01,\n",
       "          4.0796e-01, -2.7344e-01,  6.9922e-01, -3.4668e-01, -4.6240e-01,\n",
       "         -1.7175e-01,  5.0342e-01,  3.6670e-01,  1.4441e-01, -2.2290e-01,\n",
       "          7.0850e-01,  7.2998e-01,  3.4033e-01, -5.5176e-01,  3.6957e-02,\n",
       "          2.7930e-01, -2.7124e-01, -7.6111e-02, -9.1211e-01,  5.0171e-02,\n",
       "         -1.3342e-01, -7.0312e-01, -3.0350e-02, -8.6572e-01, -6.8115e-01,\n",
       "          3.5059e-01,  1.4075e-01, -5.4688e+00, -2.5009e-02,  3.3398e-01,\n",
       "          4.8645e-02,  7.4951e-02,  2.1191e-01, -9.1504e-01, -1.7322e-01,\n",
       "          1.9580e-01, -1.1823e-01, -9.4580e-01,  3.4277e-01, -1.0176e+00,\n",
       "         -2.9761e-01, -2.0469e+00, -2.7563e-01, -2.2937e-01,  1.0706e-01,\n",
       "         -1.6321e-01, -7.5000e-01, -3.5278e-01, -1.5454e-01, -1.1365e-01,\n",
       "          2.8125e-01,  2.2485e-01, -1.5884e-02,  6.1279e-01,  3.0319e-02,\n",
       "          3.3051e-02, -8.9294e-02,  1.5027e-01, -2.2009e-01, -4.7632e-01,\n",
       "         -1.2161e-02,  1.6394e-01, -2.9370e-01, -1.2683e-01, -3.0701e-02,\n",
       "          4.2627e-01,  3.8599e-01, -1.3757e-01,  8.3105e-01,  5.3711e-01,\n",
       "          5.1367e-01,  2.3962e-01,  3.5669e-01, -3.7061e-01,  2.9761e-01,\n",
       "         -7.8064e-02, -7.2937e-02, -7.6221e-01,  5.9131e-01, -6.0400e-01,\n",
       "          1.2244e-01,  1.5613e-01,  2.0825e-01, -2.5684e-01,  1.1395e-01,\n",
       "         -2.4109e-01,  1.8530e-01,  3.0957e-01,  4.2896e-01, -9.2651e-02,\n",
       "         -4.1113e-01,  4.1870e-01,  5.0098e-01,  3.6865e-01,  2.5366e-01,\n",
       "         -2.3877e-01,  1.0205e-01,  4.7461e-01,  4.5312e-01, -3.5181e-01,\n",
       "          4.0869e-01, -1.3306e-01,  3.6572e-01,  1.7017e-01,  8.6060e-02,\n",
       "          4.5929e-02,  4.5801e-01, -5.4834e-01,  3.2764e-01, -1.9019e-01,\n",
       "          2.8638e-01,  2.2290e-01,  3.9209e-01,  3.5205e-01, -8.7354e-01,\n",
       "         -1.4786e-02,  1.3403e-01, -2.1252e-01, -3.1079e-01,  2.0142e-01,\n",
       "          1.8274e-01,  1.0370e-01, -7.8906e-01, -4.8730e-01,  1.5051e-01,\n",
       "          1.2375e-02,  4.5679e-01, -4.4849e-01, -2.5177e-02, -4.4141e-01,\n",
       "         -3.4375e-01,  2.3071e-01,  5.5450e-02, -2.4329e-01,  3.7427e-01,\n",
       "          2.5415e-01,  6.0425e-02,  2.9248e-01,  2.7246e-01,  1.9913e-02,\n",
       "         -2.4341e-01, -5.8411e-02, -8.4351e-02,  3.0167e-02,  4.5349e-02,\n",
       "          5.2887e-02, -6.4148e-02,  1.7908e-01,  2.9785e-01,  3.1525e-02,\n",
       "         -6.1951e-02,  1.5564e-01,  1.3330e-01, -7.3779e-01, -2.5244e-01,\n",
       "          3.0347e-01,  9.0723e-01, -2.6074e-01, -4.3066e-01,  2.2263e-02,\n",
       "         -3.1689e-01,  3.0502e-02,  6.3477e-02, -1.9421e-01, -1.7456e-01,\n",
       "         -5.9863e-01, -4.9133e-02,  8.8135e-02,  2.4255e-01, -2.2351e-01,\n",
       "          2.8662e-01,  4.4019e-01, -1.4331e-01,  1.2451e-01,  5.6152e-01,\n",
       "         -3.9575e-01, -1.3892e-01, -1.2354e-01, -1.3110e-01, -1.6748e-01,\n",
       "          2.0435e-01,  3.0054e-01,  6.8542e-02, -1.6083e-02,  1.9897e-01,\n",
       "         -2.3071e-01,  1.6272e-01,  1.1011e-01,  1.8494e-01, -9.1839e-04,\n",
       "          2.9541e-01, -9.0027e-02, -2.4744e-01,  2.2832e+00,  2.7026e-01,\n",
       "          8.2886e-02, -3.9276e-02, -2.5610e-01,  9.7998e-01, -2.7368e-01,\n",
       "         -4.8438e-01, -2.1313e-01,  3.2080e-01, -2.1936e-01,  1.2091e-01,\n",
       "          2.1399e-01, -1.5918e-01, -2.8833e-01, -8.1421e-02, -9.1736e-02,\n",
       "          1.2103e-01, -2.5215e-03,  1.0144e-01,  2.9614e-01,  1.5222e-01,\n",
       "         -6.9092e-02,  4.8065e-02,  3.1909e-01, -1.5759e-01, -4.4769e-02,\n",
       "         -2.1332e-02, -9.6777e-01, -3.0884e-02, -1.1896e-01, -3.7158e-01,\n",
       "          3.2690e-01, -3.4302e-02, -3.1714e-01,  1.4624e-01, -5.1422e-02,\n",
       "          3.7817e-01,  4.9023e-01, -1.2476e-01,  5.3406e-03,  9.4604e-02,\n",
       "         -3.2275e-01, -1.8433e-01, -4.1553e-01,  3.3447e-01,  6.6992e-01,\n",
       "         -3.0469e-01, -3.7061e-01,  1.6528e-01,  2.1619e-01, -1.2280e-01,\n",
       "          2.2180e-01,  3.5938e-01,  8.3203e-01, -4.3359e-01, -1.3367e-01,\n",
       "          1.5320e-01,  6.1178e-04, -3.5010e-01, -1.2558e-02, -6.9531e-01,\n",
       "          2.4268e-01,  7.0312e-02, -9.8267e-02, -1.2128e-01,  3.9575e-01,\n",
       "         -3.5620e-01,  1.9556e-01, -6.2061e-01, -4.7705e-01, -4.8145e-01,\n",
       "          1.5002e-01, -3.0493e-01, -1.7242e-02, -2.6782e-01,  1.8359e-01,\n",
       "          2.5928e-01, -2.2598e-02,  3.8892e-01,  4.7559e-01,  3.1909e-01,\n",
       "         -1.2622e-01, -7.5586e-01,  8.0795e-03,  2.6489e-01, -2.3328e-01,\n",
       "          1.6455e-01,  7.4463e-02, -1.3953e-01,  3.5254e-01, -3.1323e-01,\n",
       "          1.0156e-01,  3.8794e-01,  1.3684e-01, -3.2642e-01, -3.3130e-01,\n",
       "         -9.1492e-02, -8.9111e-01, -3.3496e-01,  1.1035e-01, -7.5623e-02,\n",
       "          9.0942e-02, -1.6467e-01, -3.1519e-01, -9.4604e-02,  4.0918e-01,\n",
       "          7.9773e-02, -1.7993e-01, -3.5181e-01,  6.8176e-02,  4.2700e-01,\n",
       "         -1.4697e-01, -4.4531e-01, -3.8330e-01, -3.6768e-01,  1.2042e-01,\n",
       "         -3.4497e-01,  9.4189e-01, -2.4207e-01,  5.1074e-01, -1.2915e-01,\n",
       "         -1.0907e-01, -5.2185e-02, -1.8445e-01, -7.3486e-02,  5.3635e-03,\n",
       "          4.8071e-01,  1.8628e-01,  3.5425e-01, -5.0049e-01, -2.8979e-01,\n",
       "         -5.9912e-01, -4.0601e-01,  4.5166e-01,  2.0312e-01, -1.9739e-01,\n",
       "         -4.8828e-03,  1.4221e-02,  2.3474e-01, -2.3657e-01,  3.5950e-02,\n",
       "          4.3433e-01, -1.6882e-01,  1.6199e-01, -9.3323e-02,  4.1528e-01,\n",
       "         -2.3914e-01, -8.1543e-02, -4.6112e-02,  3.2227e-01,  3.4082e-01,\n",
       "          1.5613e-01,  6.2378e-02, -1.4331e-01,  3.1403e-02,  6.6711e-02,\n",
       "          5.1562e-01, -1.3252e-02, -3.4497e-01,  1.1249e-01,  1.1551e-02,\n",
       "         -3.9966e-01,  2.6221e-01,  5.3662e-01,  6.7725e-01, -2.7515e-01,\n",
       "         -2.3303e-01,  9.7107e-02, -1.3580e-02, -9.2725e-01,  8.8257e-02,\n",
       "          3.0908e-01,  9.0527e-01,  3.7207e-01,  7.2754e-02, -7.9834e-02,\n",
       "         -3.1812e-01,  5.4382e-02,  6.5186e-01,  3.1323e-01, -2.7206e-02,\n",
       "         -2.9953e-02,  7.9407e-02,  3.5083e-01, -4.9316e-02,  7.4768e-02,\n",
       "          3.9459e-02,  1.2674e-03, -1.0706e-01,  1.9324e-01, -3.2202e-01,\n",
       "         -5.3564e-01, -5.0146e-01,  2.0020e-01, -8.4961e-01, -1.6211e-01,\n",
       "         -5.6934e-01,  4.4409e-01, -1.2103e-01, -6.9824e-01, -3.3374e-01,\n",
       "         -4.1333e-01,  3.3325e-01, -2.4072e-01,  3.2544e-01,  3.0566e-01,\n",
       "         -1.4685e-01, -3.3203e-01, -3.1235e-02, -1.0333e-01,  2.4597e-01,\n",
       "          6.6223e-02, -2.2034e-01, -3.9648e-01,  2.1680e-01,  7.2327e-03,\n",
       "         -4.0308e-01,  3.9990e-01,  2.1106e-01,  1.3538e-01,  3.6621e-01,\n",
       "         -4.8438e-01, -3.4570e-01, -3.5864e-01,  3.7939e-01,  3.2886e-01,\n",
       "          3.0908e-01,  3.8757e-03,  3.0396e-01,  1.5613e-01, -1.2854e-01,\n",
       "          2.9224e-01,  2.5894e-02, -2.5452e-02, -5.1318e-01, -1.2964e-01,\n",
       "          4.8047e-01,  2.6929e-01,  2.9272e-01, -1.5698e-01,  1.0567e-02,\n",
       "         -4.3042e-01,  4.9707e-01,  1.1212e-01,  5.7617e-01, -4.0820e-01,\n",
       "          1.0876e-01, -4.5459e-01,  3.8818e-01,  9.4788e-02,  6.0791e-01,\n",
       "         -4.8755e-01,  1.9641e-01], dtype=torch.float16, requires_grad=True),\n",
       " ['A group of people stand in the back of a truck filled with cotton.',\n",
       "  'Men are standing on and about a truck carrying a white substance.',\n",
       "  'A group of people are standing on a pile of wool in a truck.',\n",
       "  'A group of men are loading cotton onto a truck',\n",
       "  'Workers load sheared wool onto a truck.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(outpath, 'rb') as infile:\n",
    "    image_features = pickle.load(infile)\n",
    "print(f'The type of the extracted image features are: {type(image_features)}, the total validation samples are: {len(image_features)}')\n",
    "image_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated in the [paper](https://arxiv.org/pdf/2307.16525.pdf), if you intend to change the vocabulary to suit your needs, you should first handle this vocabulary as a List, For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = ['mouse', 'cow', 'tiger', 'rabbit', 'dragon', 'snake', 'horse', 'sheep', 'monkey', 'chicken', 'dog', 'pig']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following function to obtain the corresponding feature for each prompted category in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 53.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_ensemble_prompt_embeddings(\n",
    "    device: str,\n",
    "    clip_type: str,\n",
    "    entities: List[str],\n",
    "    prompt_templates: List[str],\n",
    "    outpath: str,\n",
    "):\n",
    "    if os.path.exists(outpath):\n",
    "        with open(outpath, 'rb') as infile:\n",
    "            embeddings = pickle.load(infile)\n",
    "            return embeddings\n",
    "\n",
    "    model, _ = clip.load(clip_type, device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    for entity in tqdm(entities):\n",
    "        texts = [template.format(entity) for template in prompt_templates] # ['a picture of dog', 'photo of a dog', ...]\n",
    "        tokens = clip.tokenize(texts).to(device)               # (len_of_template, 77)\n",
    "        class_embeddings = model.encode_text(tokens).to('cpu') # (len_of_templates, clip_hidden_size)\n",
    "        class_embeddings /= class_embeddings.norm(dim = -1, keepdim = True) # (len_of_templates, clip_hidden_size)\n",
    "        class_embedding = class_embeddings.mean(dim = 0)       # (clip_hidden_size, ) \n",
    "        class_embedding /= class_embedding.norm()              # (clip_hidden_size, ) \n",
    "        embeddings.append(class_embedding)                     # [(clip_hidden_size, ), (clip_hidden_size, ), ...]\n",
    "    embeddings = torch.stack(embeddings, dim = 0).to('cpu')\n",
    "   \n",
    "    with open(outpath, 'wb') as outfile:\n",
    "        pickle.dump(embeddings, outfile)\n",
    "    return embeddings\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # prompts from CLIP\n",
    "    prompt_templates = [\n",
    "        'itap of a {}.',\n",
    "        'a bad photo of the {}.',\n",
    "        'a origami {}.',\n",
    "        'a photo of the large {}.',\n",
    "        'a {} in a video game.',\n",
    "        'art of the {}.',\n",
    "        'a photo of the small {}.'\n",
    "    ]\n",
    "  \n",
    "    outpath = './vocabulary_embedding_with_ensemble.pickle'\n",
    "    vocabulary_embeddings = generate_ensemble_prompt_embeddings(device, clip_type, vocabulary, prompt_templates, outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the generated prompted features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert vocabulary_embeddings.size()[0] == len(vocabulary)\n",
    "vocabulary_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to include your dataset and vocabulary, you need to incorporate the functions ```load_your_dataset()``` and ```load_your_vocabulary()``` into ```load_annotations.py```, following the pattern of other functions within this script.\n",
    "\n",
    "Congratulations, you have now completed the data preparation. You can proceed to train the model with your own settings. Enjoy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viecap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
